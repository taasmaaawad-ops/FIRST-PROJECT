{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taasmaaawad-ops/FIRST-PROJECT/blob/main/Evaluation-2017.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8b47345-208d-4b24-a7db-8a2e9e129651",
      "metadata": {
        "id": "a8b47345-208d-4b24-a7db-8a2e9e129651"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "27520693-9ef5-47dd-8067-0686caffbf57",
      "metadata": {
        "id": "27520693-9ef5-47dd-8067-0686caffbf57"
      },
      "outputs": [],
      "source": [
        "\n",
        "# to link folder in git hub\n",
        "# !git clone https://github.com/taasmaaawad-ops/FIRST-PROJECT.git\n",
        "# %cd FIRST-PROJECT\n",
        "#To delete folder in colab\n",
        "#!rm -r \"/content/FIRST-PROJECT/FIRST-PROJECT\"\n",
        "\n",
        "\n",
        "#import packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c0f6240-1588-4b98-9777-676e8307782c",
      "metadata": {
        "id": "5c0f6240-1588-4b98-9777-676e8307782c"
      },
      "source": [
        "# Load the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "8adc9e7b-46a7-45e4-88bc-ce714a8dacf5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "8adc9e7b-46a7-45e4-88bc-ce714a8dacf5",
        "outputId": "d95be2ac-efd9-46a0-e703-f88c10c3a927"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'FIRST-PROJECT/Dataset/samplled_data_2017.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-672960805.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"FIRST-PROJECT/Dataset/samplled_data_2017.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'FIRST-PROJECT/Dataset/samplled_data_2017.csv'"
          ]
        }
      ],
      "source": [
        "df1=pd.read_csv(r\"FIRST-PROJECT/Dataset/samplled_data_2017.csv\")\n",
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c3c1caa-ca9d-494f-a635-1efd4aec9321",
      "metadata": {
        "id": "2c3c1caa-ca9d-494f-a635-1efd4aec9321"
      },
      "outputs": [],
      "source": [
        "# Number of Unique Values\n",
        "df1.nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5280e966-b59f-4615-a2e4-268cde639c56",
      "metadata": {
        "id": "5280e966-b59f-4615-a2e4-268cde639c56"
      },
      "outputs": [],
      "source": [
        "# checking the null values\n",
        "df1.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfa9851f-a7d1-4f01-9509-1718a9378992",
      "metadata": {
        "id": "dfa9851f-a7d1-4f01-9509-1718a9378992"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "df1 = pd.read_csv(\"Dataset/samplled_data_2017.csv\")\n",
        "\n",
        "# Extract the label column\n",
        "labels = df1['Label']\n",
        "\n",
        "# Drop the label column before handling missing values\n",
        "df1.drop(columns=['Label'], inplace=True);\n",
        "\n",
        "# Fill missing values with the mean of their respective columns\n",
        "df1_cleaned = df1.fillna(df1.mean())\n",
        "\n",
        "# Add the label column back to the cleaned DataFrame\n",
        "df1_cleaned['Label'] = labels\n",
        "\n",
        "# Define the output directory to a local path\n",
        "output_dir = \"/content/FIRST-PROJECT\"\n",
        "\n",
        "# Save the cleaned DataFrame to a new CSV file in the output directory\n",
        "df1_cleaned.to_csv(output_dir + \"/cleaned_dataset_2017.csv\", index=False)\n",
        "\n",
        "print(\"Data cleaned and saved to:\", output_dir + \"/cleaned_dataset_2017.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a6bf803-8d5f-4e31-b3d8-fb995e244225",
      "metadata": {
        "id": "4a6bf803-8d5f-4e31-b3d8-fb995e244225"
      },
      "outputs": [],
      "source": [
        "df1=pd.read_csv(\"/content/FIRST-PROJECT/cleaned_dataset_2017.csv\")\n",
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a039f9d-c789-402c-978b-25db10ae3b56",
      "metadata": {
        "scrolled": true,
        "id": "9a039f9d-c789-402c-978b-25db10ae3b56"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df1 = pd.read_csv(\"/content/FIRST-PROJECT/cleaned_dataset_2017.csv\")\n",
        "\n",
        "# Get the value counts for the 'Label' column\n",
        "label_counts = df1['Label'].value_counts()\n",
        "\n",
        "# Print the value counts\n",
        "print(\"Value counts for each class:\")\n",
        "print(label_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f04f3894-02f2-42d9-a11b-e84267edb23f",
      "metadata": {
        "id": "f04f3894-02f2-42d9-a11b-e84267edb23f"
      },
      "source": [
        "# Pre-Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e4e6bf8-d957-4f01-b64f-065a7ba1b978",
      "metadata": {
        "id": "1e4e6bf8-d957-4f01-b64f-065a7ba1b978"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "df1 = pd.read_csv(\"/content/FIRST-PROJECT/cleaned_dataset_2017.csv\")\n",
        "\n",
        "# Check for missing values and handle them (replace with mean or drop, depending on your preference)\n",
        "df1 = df1.dropna()  # Drop rows with missing values\n",
        "\n",
        "# Check for infinite values and replace them with NaN\n",
        "df1.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "# Drop rows with NaN values after handling missing and infinite values\n",
        "df1 = df1.dropna()\n",
        "\n",
        "# Select only the numerical columns for normalization\n",
        "numerical_columns = df1.select_dtypes(include=['float64', 'int64']).columns\n",
        "\n",
        "# Create a copy of the DataFrame for printing the values before normalization\n",
        "original_df1 = df1.copy()\n",
        "\n",
        "# Create a StandardScaler object\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Apply Z-score normalization to the numerical columns\n",
        "df1[numerical_columns] = scaler.fit_transform(df1[numerical_columns])\n",
        "\n",
        "# Save the normalized DataFrame to a new CSV file\n",
        "normalized_output_file_path = r\"/content/FIRST-PROJECT/normalized_data_2017.csv\"\n",
        "df1.to_csv(normalized_output_file_path, index=False)\n",
        "\n",
        "# Index=False is used to prevent pandas from writing row indices to the CSV file\n",
        "\n",
        "# Print the original and normalized values\n",
        "print(\"Original Data:\")\n",
        "print(original_df1.head())  # Print the first few rows of the original DataFrame\n",
        "print(\"\\nNormalized Data:\")\n",
        "print(df1.head())  # Print the first few rows of the normalized DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a89c43e-35d3-4336-8f14-1abec42155ff",
      "metadata": {
        "id": "8a89c43e-35d3-4336-8f14-1abec42155ff"
      },
      "source": [
        "# Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45daa940-fa81-40ca-8d04-7c913e2d2d9a",
      "metadata": {
        "id": "45daa940-fa81-40ca-8d04-7c913e2d2d9a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "# Load the normalized data from the CSV file\n",
        "normalized_data_path = r\"/content/FIRST-PROJECT/normalized_data_2017.csv\"\n",
        "df1_normalized = pd.read_csv(normalized_data_path)\n",
        "\n",
        "# Separate labels from features\n",
        "labels = df1_normalized['Label']\n",
        "features = df1_normalized.drop(columns=['Label'])\n",
        "\n",
        "# Perform SVD on the feature matrix\n",
        "n_components = 10  # Number of components to keep\n",
        "svd = TruncatedSVD(n_components=n_components)\n",
        "extracted_features = svd.fit_transform(features)\n",
        "\n",
        "# Create a DataFrame for the extracted features\n",
        "df1_extracted_features = pd.DataFrame(extracted_features, columns=[f'SVD_Component_{i+1}' for i in range(n_components)])\n",
        "\n",
        "# Add the labels to the DataFrame\n",
        "df1_extracted_features['Label'] = labels\n",
        "\n",
        "# Save the extracted features with labels to a new CSV file\n",
        "extracted_features_output_path = r\"/content/FIRST-PROJECT/extracted_features_2017.csv\"\n",
        "df1_extracted_features.to_csv(extracted_features_output_path, index=False)\n",
        "\n",
        "# Print the values of the extracted features with labels\n",
        "print(\"Extracted Features with Labels:\")\n",
        "print(df1_extracted_features.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14a8085b-319f-4777-acb3-775080d6a5af",
      "metadata": {
        "id": "14a8085b-319f-4777-acb3-775080d6a5af"
      },
      "source": [
        "# Label Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86b1ccf8-d8c2-4da0-8480-b7a8073e0dc3",
      "metadata": {
        "id": "86b1ccf8-d8c2-4da0-8480-b7a8073e0dc3"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Define label encoding dictionary\n",
        "label_encoding = {\n",
        "    'BENIGN': 0,\n",
        "    'PortScan': 0,\n",
        "    'Infiltration': 1,\n",
        "    'DDoS': 1,\n",
        "    'FTP-Patator': 1,\n",
        "    'DoS GoldenEye': 0,\n",
        "    'DoS Hulk': 1,\n",
        "    'SSH-Patator': 1,\n",
        "    'DoS slowloris': 1,\n",
        "    'DoS Slowhttptest': 1,\n",
        "    'Bot': 1,\n",
        "    'Web Attack Brute Force': 1,\n",
        "    'Web Attack XSS': 1,\n",
        "    'Web Attack Sql Injection': 1,\n",
        "    'Heartbleed': 1\n",
        "}\n",
        "\n",
        "# Apply label encoding to the 'Label' column\n",
        "df1_extracted_features['Label'] = df1_extracted_features['Label'].map(label_encoding)\n",
        "\n",
        "# Print the values of the extracted features with encoded labels\n",
        "print(\"Extracted Features with Encoded Labels:\")\n",
        "print(df1_extracted_features.head())\n",
        "\n",
        "# Save the DataFrame with encoded labels to a new CSV file\n",
        "encoded_features_output_path = r\"/content/FIRST-PROJECT/encoded_features_2017.csv\"\n",
        "df1_extracted_features.to_csv(encoded_features_output_path, index=False)\n",
        "\n",
        "# Print the path where the encoded features are saved\n",
        "print(f\"Encoded features saved to: {encoded_features_output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb3452b7-a0a0-4c34-ab9a-e9a4df38d09a",
      "metadata": {
        "id": "eb3452b7-a0a0-4c34-ab9a-e9a4df38d09a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df1 = pd.read_csv(r\"/content/FIRST-PROJECT/encoded_features_2017.csv\")\n",
        "\n",
        "# Get the value counts for the 'Label' column\n",
        "label_counts = df1['Label'].value_counts()\n",
        "\n",
        "# Print the value counts\n",
        "print(\"Value counts for each class:\")\n",
        "print(label_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2310c8c7-536a-4f26-be83-07bf1e1c72d1",
      "metadata": {
        "id": "2310c8c7-536a-4f26-be83-07bf1e1c72d1"
      },
      "source": [
        "# Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62513036-e319-404a-8e85-70668c0dfb73",
      "metadata": {
        "id": "62513036-e319-404a-8e85-70668c0dfb73"
      },
      "source": [
        "# Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bbf14b3-18d2-4c9b-8a2a-4c667e95bab2",
      "metadata": {
        "id": "7bbf14b3-18d2-4c9b-8a2a-4c667e95bab2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, precision_recall_fscore_support\n",
        "\n",
        "class ThompsonSamplingMultiArmedBandit:\n",
        "    def __init__(self, n_arms):\n",
        "        self.n_arms = n_arms\n",
        "        self.alpha = np.ones(n_arms)  # Initialize alpha parameters to 1\n",
        "        self.beta = np.ones(n_arms)   # Initialize beta parameters to 1\n",
        "\n",
        "    def choose_arm(self):\n",
        "        samples = np.random.beta(self.alpha, self.beta)  # Thompson sampling\n",
        "        return np.argmax(samples)\n",
        "\n",
        "    def update(self, arm, reward):\n",
        "        if reward == 1:\n",
        "            self.alpha[arm] += 1\n",
        "        else:\n",
        "            self.beta[arm] += 1\n",
        "\n",
        "# Load the extracted features and labels from the CSV file\n",
        "extracted_features_path = r\"/content/FIRST-PROJECT/encoded_features_2017.csv\"\n",
        "df_extracted_features = pd.read_csv(extracted_features_path)\n",
        "\n",
        "# Separate features and labels\n",
        "X = df_extracted_features.drop(columns=['Label'])\n",
        "y = df_extracted_features['Label']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Multi-Armed Bandit\n",
        "n_arms = len(np.unique(y_train))  # Number of unique classes\n",
        "bandit = ThompsonSamplingMultiArmedBandit(n_arms)\n",
        "\n",
        "# Train the Multi-Armed Bandit\n",
        "for _ in range(len(X_train)):\n",
        "    arm = bandit.choose_arm()\n",
        "    reward = 1 if y_train.iloc[_] == arm else 0\n",
        "    bandit.update(arm, reward)\n",
        "\n",
        "# Initialize the Random Forest classifier\n",
        "random_forest_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the Random Forest classifier\n",
        "random_forest_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities on the testing set\n",
        "y_pred_proba = random_forest_classifier.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Threshold the probabilities to get binary predictions\n",
        "threshold = 0.5\n",
        "y_pred = (y_pred_proba > threshold).astype(int)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "classification_rep = classification_report(y_test, y_pred)\n",
        "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "# Parse classification report to get F1 score and detection rate\n",
        "classification_dict = classification_report(y_test, y_pred, output_dict=True)\n",
        "f1_score = classification_dict['1']['f1-score']\n",
        "detection_rate = classification_dict['1']['recall']\n",
        "\n",
        "# Calculate precision, recall, and F1 score\n",
        "precision, recall, _, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"F1 Score: {f1_score}\")\n",
        "print(f\"Detection Rate : {detection_rate}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"AUC Score: {auc_score}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_rep)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc83d3e5-58dd-4dae-935e-89ee63b4b0cf",
      "metadata": {
        "id": "bc83d3e5-58dd-4dae-935e-89ee63b4b0cf"
      },
      "source": [
        "# Support Vector Machine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b272d7ae-8604-4c69-a4ce-bc6ab6cdd5b1",
      "metadata": {
        "id": "b272d7ae-8604-4c69-a4ce-bc6ab6cdd5b1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, precision_recall_fscore_support\n",
        "\n",
        "class ThompsonSamplingMultiArmedBandit:\n",
        "    def __init__(self, n_arms):\n",
        "        self.n_arms = n_arms\n",
        "        self.alpha = np.ones(n_arms)  # Initialize alpha parameters to 1\n",
        "        self.beta = np.ones(n_arms)   # Initialize beta parameters to 1\n",
        "\n",
        "    def choose_arm(self):\n",
        "        samples = np.random.beta(self.alpha, self.beta)  # Thompson sampling\n",
        "        return np.argmax(samples)\n",
        "\n",
        "    def update(self, arm, reward):\n",
        "        if reward == 1:\n",
        "            self.alpha[arm] += 1\n",
        "        else:\n",
        "            self.beta[arm] += 1\n",
        "\n",
        "# Load the extracted features and labels from the CSV file\n",
        "extracted_features_path = r\"/content/FIRST-PROJECT/encoded_features_2017.csv\"\n",
        "df_extracted_features = pd.read_csv(extracted_features_path)\n",
        "\n",
        "# Separate features and labels\n",
        "X = df_extracted_features.drop(columns=['Label'])\n",
        "y = df_extracted_features['Label']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Multi-Armed Bandit\n",
        "n_arms = len(np.unique(y_train))  # Number of unique classes\n",
        "bandit = ThompsonSamplingMultiArmedBandit(n_arms)\n",
        "\n",
        "# Train the Multi-Armed Bandit\n",
        "for _ in range(len(X_train)):\n",
        "    arm = bandit.choose_arm()\n",
        "    reward = 1 if y_train.iloc[_] == arm else 0\n",
        "    bandit.update(arm, reward)\n",
        "\n",
        "# Initialize the SVM classifier\n",
        "svm_classifier = SVC(kernel='rbf', probability=True, random_state=42)\n",
        "\n",
        "# Train the SVM classifier\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities on the testing set\n",
        "y_pred_proba = svm_classifier.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Threshold the probabilities to get binary predictions\n",
        "threshold = 0.5\n",
        "y_pred = (y_pred_proba > threshold).astype(int)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "classification_rep = classification_report(y_test, y_pred)\n",
        "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "# Parse classification report to get F1 score and detection rate\n",
        "classification_dict = classification_report(y_test, y_pred, output_dict=True)\n",
        "f1_score = classification_dict['1']['f1-score']\n",
        "detection_rate = classification_dict['1']['recall']\n",
        "\n",
        "# Calculate precision, recall, and F1 score\n",
        "precision, recall, _, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"F1 Score: {f1_score}\")\n",
        "print(f\"Detection Rate : {detection_rate}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"AUC Score: {auc_score}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_rep)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c8af619-8f91-41d1-b6ad-f8e5028b75b6",
      "metadata": {
        "id": "0c8af619-8f91-41d1-b6ad-f8e5028b75b6"
      },
      "source": [
        "# LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1dd9a32-fd69-46c1-b6d8-66cf9f1acace",
      "metadata": {
        "id": "b1dd9a32-fd69-46c1-b6d8-66cf9f1acace"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, precision_recall_fscore_support\n",
        "\n",
        "class ThompsonSamplingMultiArmedBandit:\n",
        "    def __init__(self, n_arms):\n",
        "        self.n_arms = n_arms\n",
        "        self.alpha = np.ones(n_arms)  # Initialize alpha parameters to 1\n",
        "        self.beta = np.ones(n_arms)   # Initialize beta parameters to 1\n",
        "\n",
        "    def choose_arm(self):\n",
        "        samples = np.random.beta(self.alpha, self.beta)  # Thompson sampling\n",
        "        return np.argmax(samples)\n",
        "\n",
        "    def update(self, arm, reward):\n",
        "        if reward == 1:\n",
        "            self.alpha[arm] += 1\n",
        "        else:\n",
        "            self.beta[arm] += 1\n",
        "\n",
        "# Load the extracted features and labels from the CSV file\n",
        "extracted_features_path = r\"/content/FIRST-PROJECT/encoded_features_2017.csv\"\n",
        "df_extracted_features = pd.read_csv(extracted_features_path)\n",
        "\n",
        "# Separate features and labels\n",
        "X = df_extracted_features.drop(columns=['Label'])\n",
        "y = df_extracted_features['Label']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Multi-Armed Bandit\n",
        "n_arms = len(np.unique(y_train))  # Number of unique classes\n",
        "bandit = ThompsonSamplingMultiArmedBandit(n_arms)\n",
        "\n",
        "# Train the Multi-Armed Bandit\n",
        "for _ in range(len(X_train)):\n",
        "    arm = bandit.choose_arm()\n",
        "    reward = 1 if y_train.iloc[_] == arm else 0\n",
        "    bandit.update(arm, reward)\n",
        "\n",
        "# Initialize the Logistic Regression classifier\n",
        "logistic_regression_classifier = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "# Train the Logistic Regression classifier\n",
        "logistic_regression_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities on the testing set\n",
        "y_pred_proba = logistic_regression_classifier.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Threshold the probabilities to get binary predictions\n",
        "threshold = 0.5\n",
        "y_pred = (y_pred_proba > threshold).astype(int)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "classification_rep = classification_report(y_test, y_pred)\n",
        "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "# Parse classification report to get F1 score and detection rate\n",
        "classification_dict = classification_report(y_test, y_pred, output_dict=True)\n",
        "f1_score = classification_dict['1']['f1-score']\n",
        "detection_rate = classification_dict['1']['recall']\n",
        "\n",
        "# Calculate precision, recall, and F1 score\n",
        "precision, recall, _, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"F1 Score: {f1_score}\")\n",
        "print(f\"Detection Rate : {detection_rate}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"AUC Score: {auc_score}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_rep)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb9ca602-701a-46ca-a122-5cadb9b864d9",
      "metadata": {
        "id": "bb9ca602-701a-46ca-a122-5cadb9b864d9"
      },
      "source": [
        "# DNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6003efe3-b6ff-4e3b-917d-f2e965575f6f",
      "metadata": {
        "id": "6003efe3-b6ff-4e3b-917d-f2e965575f6f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "class ThompsonSamplingMultiArmedBandit:\n",
        "    def __init__(self, n_arms):\n",
        "        self.n_arms = n_arms\n",
        "        self.alpha = np.ones(n_arms)  # Initialize alpha parameters to 1\n",
        "        self.beta = np.ones(n_arms)   # Initialize beta parameters to 1\n",
        "\n",
        "    def choose_arm(self):\n",
        "        samples = np.random.beta(self.alpha, self.beta)  # Thompson sampling\n",
        "        return np.argmax(samples)\n",
        "\n",
        "    def update(self, arm, reward):\n",
        "        if reward == 1:\n",
        "            self.alpha[arm] += 1\n",
        "        else:\n",
        "            self.beta[arm] += 1\n",
        "\n",
        "# Load the extracted features and labels from the CSV file\n",
        "extracted_features_path = r\"/content/FIRST-PROJECT/encoded_features_2017.csv\"\n",
        "df_extracted_features = pd.read_csv(extracted_features_path)\n",
        "\n",
        "# Separate features and labels\n",
        "X = df_extracted_features.drop(columns=['Label'])\n",
        "y = df_extracted_features['Label']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Multi-Armed Bandit\n",
        "n_arms = len(np.unique(y_train))  # Number of unique classes\n",
        "bandit = ThompsonSamplingMultiArmedBandit(n_arms)\n",
        "\n",
        "# Train the Multi-Armed Bandit\n",
        "for _ in range(len(X_train)):\n",
        "    arm = bandit.choose_arm()\n",
        "    reward = 1 if y_train.iloc[_] == arm else 0\n",
        "    bandit.update(arm, reward)\n",
        "\n",
        "# Define and train a Deep Neural Network (DNN) using TensorFlow\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(n_arms, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Predict on the testing set\n",
        "y_pred_prob = model.predict(X_test)\n",
        "y_pred = np.argmax(y_pred_prob, axis=1)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "classification_rep = classification_report(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_rep)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14a9942d-4f84-4184-b253-88734d6d647d",
      "metadata": {
        "id": "14a9942d-4f84-4184-b253-88734d6d647d"
      },
      "outputs": [],
      "source": [
        "# Predict probabilities on the testing set\n",
        "y_pred_proba = model.predict(X_test)\n",
        "\n",
        "# Calculate precision, recall, F1 score, detection rate (recall), and AUC score\n",
        "precision_recall_f1 = classification_report(y_test, np.argmax(y_pred_proba, axis=1), output_dict=True)\n",
        "precision = precision_recall_f1['1']['precision']\n",
        "recall = precision_recall_f1['1']['recall']\n",
        "f1_score = precision_recall_f1['1']['f1-score']\n",
        "detection_rate = recall\n",
        "\n",
        "# Calculate AUC score for each class separately\n",
        "auc_scores = []\n",
        "for i in range(len(np.unique(y_train))):\n",
        "    auc_scores.append(roc_auc_score((y_test == i).astype(int), y_pred_proba[:, i]))\n",
        "\n",
        "# Average AUC scores across all classes\n",
        "auc_score = np.mean(auc_scores)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1_score}\")\n",
        "print(f\"Detection Rate (Recall): {detection_rate}\")\n",
        "print(f\"AUC Score: {auc_score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f93b3c7b-f6b3-4e4f-874a-5f399d3ee608",
      "metadata": {
        "id": "f93b3c7b-f6b3-4e4f-874a-5f399d3ee608"
      },
      "source": [
        "# IDS-Anta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4ae073a-b959-4ac3-ae9c-e8be0c76cdf0",
      "metadata": {
        "id": "f4ae073a-b959-4ac3-ae9c-e8be0c76cdf0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "class MultiArmedBanditAntColonyOptimization:\n",
        "    def __init__(self, n_arms, n_ants):\n",
        "        self.n_arms = n_arms\n",
        "        self.n_ants = n_ants\n",
        "        self.arms = [SVC(probability=True), LogisticRegression(), RandomForestClassifier(), self.create_dnn_model()]\n",
        "        self.pheromone = np.ones(n_arms)\n",
        "        self.epsilon = 0.1\n",
        "\n",
        "    def create_dnn_model(self):\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(128, activation='relu'),\n",
        "            tf.keras.layers.Dense(64, activation='relu'),\n",
        "            tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "        ])\n",
        "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    def choose_arm(self):\n",
        "        probabilities = self.pheromone / np.sum(self.pheromone)\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            return np.random.choice(self.n_arms)\n",
        "        else:\n",
        "            return np.random.choice(self.n_arms, p=probabilities)\n",
        "\n",
        "    def update(self, arm, reward):\n",
        "        self.pheromone[arm] += reward\n",
        "\n",
        "# Load the extracted features and labels from the CSV file\n",
        "extracted_features_path = r\"E:\\train\\encoded_features_2017.csv\"\n",
        "df_extracted_features = pd.read_csv(extracted_features_path)\n",
        "\n",
        "# Separate features and labels\n",
        "X = df_extracted_features.drop(columns=['Label'])\n",
        "y = df_extracted_features['Label']\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Multi-Armed Bandit with Ant Colony Optimization\n",
        "n_arms = 4  # Number of classifiers\n",
        "n_ants = 10  # Number of ants\n",
        "bandit = MultiArmedBanditAntColonyOptimization(n_arms, n_ants)\n",
        "\n",
        "# Train the Multi-Armed Bandit\n",
        "num_iterations = 10\n",
        "for _ in range(num_iterations):\n",
        "    for _ in range(n_ants):\n",
        "        arm = bandit.choose_arm()\n",
        "        classifier = bandit.arms[arm]\n",
        "        classifier.fit(X_train, y_train)\n",
        "        if classifier.__class__.__name__ != 'Sequential':\n",
        "            y_pred = classifier.predict(X_train)\n",
        "            reward = accuracy_score(y_train, y_pred)\n",
        "        else:\n",
        "            _, accuracy = classifier.evaluate(X_train, y_train, verbose=0)\n",
        "            reward = accuracy\n",
        "        bandit.update(arm, reward)\n",
        "\n",
        "# Choose the best classifier\n",
        "best_arm = np.argmax(bandit.pheromone)\n",
        "best_classifier = bandit.arms[best_arm]\n",
        "\n",
        "# Evaluate the best classifier on the test set\n",
        "if best_classifier.__class__.__name__ != 'Sequential':\n",
        "    y_pred = best_classifier.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    classification_rep = classification_report(y_test, y_pred)\n",
        "else:\n",
        "    _, accuracy = best_classifier.evaluate(X_test, y_test, verbose=0)\n",
        "    y_pred = (best_classifier.predict(X_test) > 0.5).astype(\"int32\")\n",
        "    classification_rep = classification_report(y_test, y_pred)\n",
        "\n",
        "# Print accuracy and classification report\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_rep)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "204590e4-8f0c-43a9-9a43-9ba2549bafa6",
      "metadata": {
        "id": "204590e4-8f0c-43a9-9a43-9ba2549bafa6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "class MultiArmedBanditAntColonyOptimization:\n",
        "    def __init__(self, n_arms, n_ants):\n",
        "        self.n_arms = n_arms\n",
        "        self.n_ants = n_ants\n",
        "        self.arms = [SVC(probability=True), LogisticRegression(), RandomForestClassifier(), self.create_dnn_model()]\n",
        "        self.pheromone = np.ones(n_arms)\n",
        "        self.epsilon = 0.1\n",
        "\n",
        "    def create_dnn_model(self):\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(128, activation='relu'),\n",
        "            tf.keras.layers.Dense(64, activation='relu'),\n",
        "            tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "        ])\n",
        "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    def choose_arm(self):\n",
        "        probabilities = self.pheromone / np.sum(self.pheromone)\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            return np.random.choice(self.n_arms)\n",
        "        else:\n",
        "            return np.random.choice(self.n_arms, p=probabilities)\n",
        "\n",
        "    def update(self, arm, reward):\n",
        "        self.pheromone[arm] += reward\n",
        "\n",
        "# Load the extracted features and labels from the CSV file\n",
        "extracted_features_path = r\"/content/FIRST-PROJECT/encoded_features_2017.csv\"\n",
        "df_extracted_features = pd.read_csv(extracted_features_path)\n",
        "\n",
        "# Separate features and labels\n",
        "X = df_extracted_features.drop(columns=['Label'])\n",
        "y = df_extracted_features['Label']\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Multi-Armed Bandit with Ant Colony Optimization\n",
        "n_arms = 4  # Number of classifiers\n",
        "n_ants = 10  # Number of ants\n",
        "bandit = MultiArmedBanditAntColonyOptimization(n_arms, n_ants)\n",
        "\n",
        "# Train the Multi-Armed Bandit\n",
        "num_iterations = 10\n",
        "for _ in range(num_iterations):\n",
        "    for _ in range(n_ants):\n",
        "        arm = bandit.choose_arm()\n",
        "        classifier = bandit.arms[arm]\n",
        "        classifier.fit(X_train, y_train)\n",
        "        if classifier.__class__.__name__ != 'Sequential':\n",
        "            y_pred = classifier.predict(X_train)\n",
        "            reward = accuracy_score(y_train, y_pred)\n",
        "        else:\n",
        "            _, accuracy = classifier.evaluate(X_train, y_train, verbose=0)\n",
        "            reward = accuracy\n",
        "        bandit.update(arm, reward)\n",
        "\n",
        "# Choose the best classifier\n",
        "best_arm = np.argmax(bandit.pheromone)\n",
        "best_classifier = bandit.arms[best_arm]\n",
        "\n",
        "# Evaluate the best classifier on the test set\n",
        "if best_classifier.__class__.__name__ != 'Sequential':\n",
        "    y_pred = best_classifier.predict(X_test)\n",
        "    precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
        "    auc_score = roc_auc_score(y_test, y_pred)\n",
        "else:\n",
        "    _, accuracy = best_classifier.evaluate(X_test, y_test, verbose=0)\n",
        "    y_pred = (best_classifier.predict(X_test) > 0.5).astype(\"int32\")\n",
        "    precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
        "    auc_score = roc_auc_score(y_test, y_pred)\n",
        "\n",
        "# Compute detection rate (equivalent to recall)\n",
        "detection_rate = recall\n",
        "\n",
        "# Print precision, recall, detection rate, F1 score, and AUC score\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"Detection Rate:\", detection_rate)\n",
        "print(\"F1 Score:\", f1_score)\n",
        "print(\"AUC Score:\", auc_score)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}